{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gCyEnRaX4qV",
        "outputId": "bc651ea3-416f-4bdf-ac75-b40ebd980686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62B3cooaYXpg",
        "outputId": "48294445-b65e-4e3b-f1ce-6173d6f72529"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-pn4uc0td\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-pn4uc0td\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoWDKStQZv5D",
        "outputId": "a99e63ec-602d-43f9-eb4a-a4b118e5d78d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include<cuda_runtime.h>\n",
        "\n",
        "// CUDA kernel for vector addition\n",
        "__global__ void vectorAdd(int* a, int* b, int* c, int size) \n",
        "{\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (tid < size) {\n",
        "        c[tid] = a[tid] + b[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() \n",
        "{\n",
        "    int size = 100;  // Size of the vectors\n",
        "    int* a, * b, * c;    // Host vectors\n",
        "    int* dev_a, * dev_b, * dev_c;  // Device vectors\n",
        "\n",
        "    // Allocate memory for host vectors\n",
        "    a = (int*)malloc(size * sizeof(int));\n",
        "    b = (int*)malloc(size * sizeof(int));\n",
        "    c = (int*)malloc(size * sizeof(int));\n",
        "\n",
        "    // Initialize host vectors\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        a[i] = i;\n",
        "        b[i] = 2 * i;\n",
        "        c[i] = a[i]+b[i];\n",
        "    }\n",
        "\n",
        "    // Allocate memory on the device for device vectors\n",
        "    cudaMalloc((void**)&dev_a, size * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_b, size * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_c, size * sizeof(int));\n",
        "\n",
        "    // Copy host vectors to device\n",
        "    cudaMemcpy(dev_a, a, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel for vector addition\n",
        "    int blockSize = 256;\n",
        "    int gridSize = (size + blockSize - 1) / blockSize;\n",
        "    vectorAdd<<<gridSize, blockSize>>>(dev_a, dev_b, dev_c, size);\n",
        "\n",
        "    // Copy result from device to host\n",
        "    cudaMemcpy(c, dev_c, size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print result\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        printf(\"%d + %d = %d\\n\", a[i], b[i], c[i]);\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    // Free host memory\n",
        "    free(a);\n",
        "    free(b);\n",
        "    free(c);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRatpgjicY_A",
        "outputId": "c6444fa2-8c1f-42be-d376-f4f45c98a10c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 + 0 = 0\n",
            "1 + 2 = 3\n",
            "2 + 4 = 6\n",
            "3 + 6 = 9\n",
            "4 + 8 = 12\n",
            "5 + 10 = 15\n",
            "6 + 12 = 18\n",
            "7 + 14 = 21\n",
            "8 + 16 = 24\n",
            "9 + 18 = 27\n",
            "10 + 20 = 30\n",
            "11 + 22 = 33\n",
            "12 + 24 = 36\n",
            "13 + 26 = 39\n",
            "14 + 28 = 42\n",
            "15 + 30 = 45\n",
            "16 + 32 = 48\n",
            "17 + 34 = 51\n",
            "18 + 36 = 54\n",
            "19 + 38 = 57\n",
            "20 + 40 = 60\n",
            "21 + 42 = 63\n",
            "22 + 44 = 66\n",
            "23 + 46 = 69\n",
            "24 + 48 = 72\n",
            "25 + 50 = 75\n",
            "26 + 52 = 78\n",
            "27 + 54 = 81\n",
            "28 + 56 = 84\n",
            "29 + 58 = 87\n",
            "30 + 60 = 90\n",
            "31 + 62 = 93\n",
            "32 + 64 = 96\n",
            "33 + 66 = 99\n",
            "34 + 68 = 102\n",
            "35 + 70 = 105\n",
            "36 + 72 = 108\n",
            "37 + 74 = 111\n",
            "38 + 76 = 114\n",
            "39 + 78 = 117\n",
            "40 + 80 = 120\n",
            "41 + 82 = 123\n",
            "42 + 84 = 126\n",
            "43 + 86 = 129\n",
            "44 + 88 = 132\n",
            "45 + 90 = 135\n",
            "46 + 92 = 138\n",
            "47 + 94 = 141\n",
            "48 + 96 = 144\n",
            "49 + 98 = 147\n",
            "50 + 100 = 150\n",
            "51 + 102 = 153\n",
            "52 + 104 = 156\n",
            "53 + 106 = 159\n",
            "54 + 108 = 162\n",
            "55 + 110 = 165\n",
            "56 + 112 = 168\n",
            "57 + 114 = 171\n",
            "58 + 116 = 174\n",
            "59 + 118 = 177\n",
            "60 + 120 = 180\n",
            "61 + 122 = 183\n",
            "62 + 124 = 186\n",
            "63 + 126 = 189\n",
            "64 + 128 = 192\n",
            "65 + 130 = 195\n",
            "66 + 132 = 198\n",
            "67 + 134 = 201\n",
            "68 + 136 = 204\n",
            "69 + 138 = 207\n",
            "70 + 140 = 210\n",
            "71 + 142 = 213\n",
            "72 + 144 = 216\n",
            "73 + 146 = 219\n",
            "74 + 148 = 222\n",
            "75 + 150 = 225\n",
            "76 + 152 = 228\n",
            "77 + 154 = 231\n",
            "78 + 156 = 234\n",
            "79 + 158 = 237\n",
            "80 + 160 = 240\n",
            "81 + 162 = 243\n",
            "82 + 164 = 246\n",
            "83 + 166 = 249\n",
            "84 + 168 = 252\n",
            "85 + 170 = 255\n",
            "86 + 172 = 258\n",
            "87 + 174 = 261\n",
            "88 + 176 = 264\n",
            "89 + 178 = 267\n",
            "90 + 180 = 270\n",
            "91 + 182 = 273\n",
            "92 + 184 = 276\n",
            "93 + 186 = 279\n",
            "94 + 188 = 282\n",
            "95 + 190 = 285\n",
            "96 + 192 = 288\n",
            "97 + 194 = 291\n",
            "98 + 196 = 294\n",
            "99 + 198 = 297\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include<cuda_runtime.h>\n",
        "\n",
        "// CUDA kernel for matrix multiplication\n",
        "__global__ void matrixMul(int* a, int* b, int* c, int rowsA, int colsA, int colsB) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int sum = 0;\n",
        "    if (row < rowsA && col < colsB) {\n",
        "        for (int i = 0; i < colsA; i++) {\n",
        "            sum += a[row * colsA + i] * b[i * colsB + col];\n",
        "        }\n",
        "        c[row * colsB + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int rowsA = 10;  // Rows of matrix A\n",
        "    int colsA = 10;  // Columns of matrix A\n",
        "    int rowsB = colsA; // Rows of matrix B\n",
        "    int colsB = 10;  // Columns of matrix B\n",
        "\n",
        "    int* a, * b, * c;  // Host matrices\n",
        "    int* dev_a, * dev_b, * dev_c;  // Device matrices\n",
        "\n",
        "    // Allocate memory for host matrices\n",
        "    a = (int*)malloc(rowsA * colsA * sizeof(int));\n",
        "    b = (int*)malloc(rowsB * colsB * sizeof(int));\n",
        "    c = (int*)malloc(rowsA * colsB * sizeof(int));\n",
        "\n",
        "    // Initialize host matrices\n",
        "    for (int i = 0; i < rowsA * colsA; i++) {\n",
        "        a[i] = i;\n",
        "    }\n",
        "    for (int i = 0; i < rowsB * colsB; i++) {\n",
        "        b[i] = 2 * i;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on the device for device matrices\n",
        "    cudaMalloc((void**)&dev_a, rowsA * colsA * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_b, rowsB * colsB * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_c, rowsA * colsB * sizeof(int));\n",
        "\n",
        "    // Copy host matrices to device\n",
        "    cudaMemcpy(dev_a, a, rowsA * colsA * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, rowsB * colsB * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define grid and block dimensions\n",
        "    dim3 blockSize(16, 16);\n",
        "    dim3 gridSize((colsB + blockSize.x - 1) / blockSize.x, (rowsA + blockSize.y - 1) / blockSize.y);\n",
        "\n",
        "    // Launch kernel for matrix multiplication\n",
        "    matrixMul<<<gridSize, blockSize>>>(dev_a, dev_b, dev_c, rowsA, colsA, colsB);\n",
        "\n",
        "    // Copy result from device to host\n",
        "    cudaMemcpy(c, dev_c, rowsA * colsB * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print result\n",
        "    printf(\"Result:\\n\");\n",
        "    for (int i = 0; i < rowsA; i++) {\n",
        "        for (int j = 0; j < colsB; j++) {\n",
        "            printf(\"%d \", c[i* colsB + j ]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    // Free host memory\n",
        "    free(a);\n",
        "    free(b);\n",
        "    free(c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW3UkTu_Wy-d",
        "outputId": "f6a44efc-de9a-4651-f649-f6cd2543961e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result:\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}